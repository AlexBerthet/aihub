{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add another:\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f398ed3b860>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sigmoid layer:\n",
    "layers.Dense(64, activation='sigmoid')\n",
    "# Or:\n",
    "layers.Dense(64, activation=tf.sigmoid)\n",
    "\n",
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "layers.Dense(64, bias_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with a kernel initialized to a random orthogonal matrix:\n",
    "layers.Dense(64, kernel_initializer='orthogonal')\n",
    "# A linear layer with a bias vector initialized to 2.0s:\n",
    "layers.Dense(64, bias_initializer=keras.initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 432us/step - loss: 11.6094 - acc: 0.1000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 11.5703 - acc: 0.1100\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 11.5616 - acc: 0.1130\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 11.5550 - acc: 0.1250\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 11.5512 - acc: 0.1410\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 11.5452 - acc: 0.1430\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 11.5411 - acc: 0.1450\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 11.5359 - acc: 0.1500\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 11.5324 - acc: 0.1570\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 11.5276 - acc: 0.1800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f398ed3b978>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input tf.data datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 11.5245 - acc: 0.1677 - val_loss: 11.3540 - val_acc: 0.1146\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 11.5116 - acc: 0.1750 - val_loss: 11.4767 - val_acc: 0.1146\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 11.4998 - acc: 0.1792 - val_loss: 11.5549 - val_acc: 0.0521\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 11.5077 - acc: 0.1927 - val_loss: 11.2224 - val_acc: 0.1146\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4739 - acc: 0.1979 - val_loss: 11.6383 - val_acc: 0.0938\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4914 - acc: 0.1906 - val_loss: 11.6238 - val_acc: 0.0938\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4766 - acc: 0.2042 - val_loss: 11.5329 - val_acc: 0.1042\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4766 - acc: 0.1885 - val_loss: 11.5519 - val_acc: 0.1562\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4753 - acc: 0.1969 - val_loss: 11.4732 - val_acc: 0.1354\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 11.4759 - acc: 0.2042 - val_loss: 11.7832 - val_acc: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f397a2acba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Validation data\n",
    "\n",
    "val_data = np.random.random((1000, 32))\n",
    "val_labels = np.random.random((1000, 10))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "model.fit(\n",
    "    dataset, \n",
    "    epochs=10,\n",
    "    steps_per_epoch=30,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 72us/step\n",
      "rel [11.53584161376953, 0.09]\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "rel [11.479800287882487, 0.21770833333333334]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09687859, 0.10937908, 0.11363962, ..., 0.08654143, 0.10548483,\n",
       "        0.10896184],\n",
       "       [0.08477022, 0.09555576, 0.11435264, ..., 0.09007747, 0.11638239,\n",
       "        0.10527991],\n",
       "       [0.08978126, 0.0976119 , 0.0967271 , ..., 0.10206714, 0.10437228,\n",
       "        0.10218671],\n",
       "       ...,\n",
       "       [0.09892099, 0.09305961, 0.12870906, ..., 0.12323888, 0.08553463,\n",
       "        0.10478143],\n",
       "       [0.11029734, 0.08287518, 0.11543798, ..., 0.11569832, 0.09153935,\n",
       "        0.10177524],\n",
       "       [0.0782255 , 0.11362751, 0.11057317, ..., 0.10504314, 0.10654626,\n",
       "        0.09850879]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake data to evaluate\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 10))\n",
    "\n",
    "rel = model.evaluate(x, y, batch_size=32)\n",
    "print(\"rel\", rel)\n",
    "rel = model.evaluate(dataset, steps=30)\n",
    "print(\"rel\", rel)\n",
    "\n",
    "\n",
    "# Predict\n",
    "model.predict(x, batch_size=32)\n",
    "model.predict(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build advanced models with the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 307us/step - loss: 11.7025 - acc: 0.1000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 11.6092 - acc: 0.1100\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 11.5861 - acc: 0.0950\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 11.5730 - acc: 0.0970\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 11.5638 - acc: 0.1170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f39970cdda0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,))  # Returns a placeholder tensor\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Instantiate the model given inputs and outputs.\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 369us/step - loss: 11.7069 - acc: 0.1030\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 11.6606 - acc: 0.1040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 11.6043 - acc: 0.1020\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 11.5811 - acc: 0.1240\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 11.5732 - acc: 0.1230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f39943c8e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(MyModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.dense_1 = keras.layers.Dense(32, activation='relu')\n",
    "    self.dense_2 = keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define your forward pass here,\n",
    "    # using layers you previously defined (in `__init__`).\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    # You need to override this function if you want to use the subclassed model\n",
    "    # as part of a functional-style model.\n",
    "    # Otherwise, this method is optional.\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.num_classes\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "\n",
    "# Instantiates the subclassed model.\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 168us/step - loss: 13.0382 - acc: 0.1020\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 12.4757 - acc: 0.0960\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 12.1355 - acc: 0.1040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 12.0485 - acc: 0.1020\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 12.0150 - acc: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3997099630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    self.output_dim = output_dim\n",
    "    super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "    # Create a trainable weight variable for this layer.\n",
    "    self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=shape,\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "    # Be sure to call this at the end\n",
    "    super(MyLayer, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "  \"\"\"\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.output_dim\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super(MyLayer, self).get_config()\n",
    "    base_config['output_dim'] = self.output_dim\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "# Create a model using the custom layer\n",
    "model = keras.Sequential([MyLayer(10),\n",
    "                          keras.layers.Activation('softmax')])\n",
    "\n",
    "# The compile step specifies the training configuration\n",
    "model.compile(\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Trains for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback is an object passed to a model to customize and extend its behavior during training. You can write your own custom callback, or use the built-in tf.keras.callbacks that include:\n",
    "\n",
    "    *tf.keras.callbacks.ModelCheckpoint: Save checkpoints of your model at regular intervals.\n",
    "*tf.keras.callbacks.LearningRateScheduler: Dynamically change the learning rate.\n",
    "*tf.keras.callbacks.EarlyStopping: Interrupt training when validation performance has stopped improving.\n",
    "*tf.keras.callbacks.TensorBoard: Monitor the model's behavior using TensorBoard.\n",
    "\n",
    "To use a tf.keras.callbacks.Callback, pass it to the model's fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 248us/step - loss: 11.4812 - acc: 0.2080 - val_loss: 11.5448 - val_acc: 0.0850\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 98us/step - loss: 11.4780 - acc: 0.2140 - val_loss: 11.5438 - val_acc: 0.1000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 11.4775 - acc: 0.2170 - val_loss: 11.5460 - val_acc: 0.1030\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 11.4691 - acc: 0.2110 - val_loss: 11.5507 - val_acc: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3998844a58>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "  keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=32, epochs=50, callbacks=callbacks,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f39786bf9b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights to a TensorFlow Checkpoint file\n",
    "model.save_weights('./my_model')\n",
    "# Restore the model's state,\n",
    "# this requires a model with the same architecture.\n",
    "model.load_weights('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py when saving in hdf5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-feb7e72f5d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save weights to a HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Restore the model's state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       raise ImportError(\n\u001b[0;32m-> 1347\u001b[0;31m           '`save_weights` requires h5py when saving in hdf5.')\n\u001b[0m\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       \u001b[0mcheck_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py when saving in hdf5."
     ]
    }
   ],
   "source": [
    "# Save weights to a HDF5 file\n",
    "model.save_weights('my_model.h5', save_format='h5')\n",
    "\n",
    "# Restore the model's state\n",
    "model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.models' has no attribute 'from_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-25963bd32e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Recreate the model (freshly initialized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfresh_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Serializes a model to YAML format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0myaml_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.models' has no attribute 'from_json'"
     ]
    }
   ],
   "source": [
    "# Serialize a model to JSON format\n",
    "json_string = model.to_json()\n",
    "# Recreate the model (freshly initialized)\n",
    "fresh_model = keras.models.from_json(json_string)\n",
    "# Serializes a model to YAML format\n",
    "yaml_string = model.to_yaml()\n",
    "# Recreate the model\n",
    "fresh_model = keras.models.from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 414us/step - loss: 11.5705 - acc: 0.1010\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 11.5613 - acc: 0.1110\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 68us/step - loss: 11.5599 - acc: 0.1150\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 11.5593 - acc: 0.1160\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 11.5589 - acc: 0.1100\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-51acafc6e9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save entire model to a HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Recreate the exact same model, including weights and optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_model` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "# Create a trivial model\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Dense(10, activation='softmax', input_shape=(32,)),\n",
    "  keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, batch_size=32, epochs=5)\n",
    "\n",
    "\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model = keras.models.load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
